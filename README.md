### 计算机知识

----
- [设计模式](notes/设计模式.md)



### 每日小段子
----
- [mean normalization ](#)均值归一化，特征缩放(feature scalling)的一种方法，目的是为了梯度下降(Gradient Descent)的更快，收敛所需迭代次数更少！
- [gradient descent](#) 梯度下降的正确运行
要确保梯度下降算法正确运行，需要保证 J(θ)在每一步迭代中都减小，如果某一步减少的值少于某个很小的值 ϵ , 则其收敛。如果梯度下降算法不能正常运行，考虑使用更小的步长α，这里需要注意两点：
1）对于足够小的α,  J(θ)能保证在每一步都减小；
2）但是如果α太小，梯度下降算法收敛的会很慢，意味着需要大量的迭代次数；
总结：
1）如果α太小，就会收敛很慢；
2）如果α太大，就不能保证每一次迭代J(θ)都减小，也就不能保证J(θ)收敛；
如何选择α-经验的方法：
..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1...
约3倍于前一个数。

- 